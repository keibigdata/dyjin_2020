{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.layers import Dense, Concatenate,Dropout, Input, Embedding, LSTM, Bidirectional,Flatten,SpatialDropout1D, CuDNNLSTM ,Conv1D,GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,SpatialDropout1D\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing import sequence\n",
    "from random import *\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,SpatialDropout1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import copy\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "n = 150000\n",
    "model = load_model('./c_model/c_senti_model1.h5')\n",
    "\n",
    "pos_tagger1 = Mecab()\n",
    "pos_tagger2 = Twitter()\n",
    "\n",
    "pos_tagger = pos_tagger1\n",
    "\n",
    "max_slen1 = 200\n",
    "max_slen2 = 500\n",
    "max_slen3 = 200\n",
    "max_slen4 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 \n",
    "\n",
    "with open('./c_model/tk1_1.pickle', 'rb') as handle:\n",
    "    tk1 = pickle.load(handle)\n",
    "        \n",
    "with open('./c_model/tk2_1.pickle', 'rb') as handle:\n",
    "    tk2 = pickle.load(handle)\n",
    "    \n",
    "with open('./c_model/tk3_1.pickle', 'rb') as handle:\n",
    "    tk3 = pickle.load(handle)\n",
    "    \n",
    "with open('./c_model/tk4_1.pickle', 'rb') as handle:\n",
    "    tk4 = pickle.load(handle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CODE, CHOSUNG, JUNGSUNG = 44032, 588, 28\n",
    "\n",
    "# 초성 리스트. 00 ~ 18\n",
    "CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "# 중성 리스트. 00 ~ 20\n",
    "JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "# 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "def convert(test_keyword):\n",
    "    split_keyword_list = list(test_keyword)\n",
    "    #print(split_keyword_list)\n",
    "\n",
    "    result = list()\n",
    "    for keyword in split_keyword_list:\n",
    "        # 한글 여부 check 후 분리\n",
    "       # print(keyword)\n",
    "        if re.match('.*[ㄱ-ㅎㅏ-ㅣ가-힣]+.*', keyword) is not None:\n",
    "            try:\n",
    "                char_code = ord(keyword) - BASE_CODE\n",
    "                char1 = int(char_code / CHOSUNG)\n",
    "                if char1==0:  \n",
    "                    result.append('')\n",
    "                result.append(CHOSUNG_LIST[char1])\n",
    "               # print('초성 : {}'.format(CHOSUNG_LIST[char1]))\n",
    "                char2 = int((char_code - (CHOSUNG * char1)) / JUNGSUNG)\n",
    "                if char2==0:\n",
    "                    result.append('')\n",
    "                result.append(JUNGSUNG_LIST[char2])\n",
    "              #  print('중성 : {}'.format(JUNGSUNG_LIST[char2]))\n",
    "                char3 = int((char_code - (CHOSUNG * char1) - (JUNGSUNG * char2)))\n",
    "                if char3==0:\n",
    "                    result.append('')\n",
    "                else:\n",
    "                    result.append(JONGSUNG_LIST[char3])\n",
    "               # print('종성 : {}'.format(JONGSUNG_LIST[char3]))\n",
    "            except:\n",
    "                result.append(keyword)\n",
    "        else:\n",
    "            result.append(keyword)\n",
    "    # result\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 부정 댓글 확률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./neg_df.csv\")\n",
    "X_test = test['내용']\n",
    "X_test = X_test.drop_duplicates()\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "idx_list = []\n",
    "\n",
    "# 형태소 분석이 어려운 샘플은 제외\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "    \n",
    "        pos = list(pd.DataFrame(pos_tagger1.pos(t))[1])\n",
    "        pos2 = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "        \n",
    "        if len(pos) == 0 or len(pos2) == 0:\n",
    "            print(str(idx)+\"오류 제외\")\n",
    "        else :\n",
    "            idx_list.append(idx)\n",
    "    except:\n",
    "        print(str(idx)+\"오류 제외\")\n",
    "        \n",
    "X_test = X_test[idx_list].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        # 조사 제거\n",
    "        pos = list(pd.DataFrame(pos_tagger1.pos(t))[1])\n",
    "        t = np.array(pos_tagger.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))\n",
    "        \n",
    "    except:\n",
    "        print(t)\n",
    "        print(str(idx)+\" 제외\")\n",
    "        \n",
    "X_test1 = t_result_list\n",
    "\n",
    "\n",
    "t_result_list = []\n",
    "for i,s in enumerate(X_test):\n",
    "    s = str(s)\n",
    "    s = s.strip()\n",
    "    temp_list = s.split(\" \")\n",
    "    \n",
    "   # 특수문자 제거  \n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('\\'', '', s)\n",
    "    s = re.sub('\\「', '', s)\n",
    "    s = re.sub('\\」', '', s)\n",
    "    s = re.sub('\\,', '', s)\n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('@', '', s)\n",
    "    s = re.sub('\\+', '', s)\n",
    "    s = re.sub('[A-z]', '', s)\n",
    "    s = re.sub('[0-9]', '', s)\n",
    "    s = s.strip()\n",
    "    \n",
    "    t_result_list.append(s)\n",
    "\n",
    "X_all_list2 = t_result_list\n",
    "\n",
    "for i, comment in enumerate(X_all_list2):\n",
    "    temp = convert(comment)\n",
    "    \n",
    "    # 빈칸제거\n",
    "    temp = temp.replace(\" \",\"\")\n",
    "    temp2 = \"\"\n",
    "    for t in temp:\n",
    "        temp2 = temp2 + t +  \" \"\n",
    "    temp2 = temp2[0:-1]\n",
    "    X_all_list2[i] = temp2\n",
    "\n",
    "X_test2 = X_all_list2\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        t = t.strip()\n",
    "        \n",
    "        # 조사 제거\n",
    "        t = t.replace(\" \", \"\") \n",
    "        t = list(t)\n",
    "        t_result_list.append(\" \".join(t))\n",
    "        \n",
    "    except:\n",
    "        print(str(idx)+\" 제외\")\n",
    "\n",
    "X_test3 = t_result_list\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        # 조사 제거\n",
    "        pos = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "        t = np.array(pos_tagger2.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))        \n",
    "    except:\n",
    "        print(t)\n",
    "        #print(str(idx)+\" 제외\")\n",
    "\n",
    "X_test4 = t_result_list\n",
    "\n",
    "X_test1 = tk1.texts_to_sequences(X_test1)\n",
    "X_test1 = sequence.pad_sequences(X_test1, maxlen=max_slen1,padding='pre')\n",
    "X_test2 = tk2.texts_to_sequences(X_test2)\n",
    "X_test2 = sequence.pad_sequences(X_test2, maxlen=max_slen2,padding='pre')\n",
    "X_test3 = tk3.texts_to_sequences(X_test3)\n",
    "X_test3 = sequence.pad_sequences(X_test3, maxlen=max_slen3,padding='pre')\n",
    "X_test4 = tk4.texts_to_sequences(X_test4)\n",
    "X_test4 = sequence.pad_sequences(X_test4, maxlen=max_slen4,padding='pre')\n",
    "\n",
    "test_result = model.predict([X_test1,X_test2,X_test3,X_test4])\n",
    "neg_prob = pd.DataFrame(test_result)[1]\n",
    "\n",
    "test['prob'] = neg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/compat/_optional.py:106: UserWarning: Pandas requires version '0.9.8' or newer of 'xlsxwriter' (version '0.7.3' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "test = test.sort_values(['prob'],ascending=False)\n",
    "test = test.reset_index(drop=True)\n",
    "neg_test_result_df = test.loc[0:(n-1),['내용','prob']]\n",
    "neg_test_result_df.to_excel('./sub_neg_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 긍정 댓글 확률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./pos_df.csv\")\n",
    "X_test = test['내용']\n",
    "X_test = X_test.drop_duplicates()\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "idx_list = []\n",
    "\n",
    "# 형태소 분석이 어려운 샘플은 제외\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "    \n",
    "        pos = list(pd.DataFrame(pos_tagger1.pos(t))[1])\n",
    "        pos2 = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "        \n",
    "        if len(pos) == 0 or len(pos2) == 0:\n",
    "            print(str(idx)+\"오류 제외\")\n",
    "        else :\n",
    "            idx_list.append(idx)\n",
    "    except:\n",
    "        print(str(idx)+\"오류 제외\")\n",
    "        \n",
    "X_test = X_test[idx_list].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        # 조사 제거\n",
    "        pos = list(pd.DataFrame(pos_tagger.pos(t))[1])\n",
    "        t = np.array(pos_tagger.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))\n",
    "        \n",
    "    except:\n",
    "        print(t)\n",
    "        print(str(idx)+\" 제외\")\n",
    "        \n",
    "X_test1 = t_result_list\n",
    "\n",
    "\n",
    "t_result_list = []\n",
    "for i,s in enumerate(X_test):\n",
    "    s = str(s)\n",
    "    s = s.strip()\n",
    "    temp_list = s.split(\" \")\n",
    "    \n",
    "   # 특수문자 제거  \n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('\\'', '', s)\n",
    "    s = re.sub('\\「', '', s)\n",
    "    s = re.sub('\\」', '', s)\n",
    "    s = re.sub('\\,', '', s)\n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('@', '', s)\n",
    "    s = re.sub('\\+', '', s)\n",
    "    s = re.sub('[A-z]', '', s)\n",
    "    s = re.sub('[0-9]', '', s)\n",
    "    s = s.strip()\n",
    "    \n",
    "    t_result_list.append(s)\n",
    "\n",
    "X_all_list2 = t_result_list\n",
    "\n",
    "for i, comment in enumerate(X_all_list2):\n",
    "    temp = convert(comment)\n",
    "    \n",
    "    # 빈칸제거\n",
    "    temp = temp.replace(\" \",\"\")\n",
    "    temp2 = \"\"\n",
    "    for t in temp:\n",
    "        temp2 = temp2 + t +  \" \"\n",
    "    temp2 = temp2[0:-1]\n",
    "    X_all_list2[i] = temp2\n",
    "\n",
    "X_test2 = X_all_list2\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        t = t.strip()\n",
    "        \n",
    "        # 조사 제거\n",
    "        t = t.replace(\" \", \"\") \n",
    "        t = list(t)\n",
    "        t_result_list.append(\" \".join(t))\n",
    "        \n",
    "    except:\n",
    "        print(str(idx)+\" 제외\")\n",
    "\n",
    "X_test3 = t_result_list\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        # 조사 제거\n",
    "        pos = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "        t = np.array(pos_tagger2.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))        \n",
    "    except:\n",
    "        print(t)\n",
    "        #print(str(idx)+\" 제외\")\n",
    "\n",
    "X_test4 = t_result_list\n",
    "\n",
    "X_test1 = tk1.texts_to_sequences(X_test1)\n",
    "X_test1 = sequence.pad_sequences(X_test1, maxlen=max_slen1,padding='pre')\n",
    "X_test2 = tk2.texts_to_sequences(X_test2)\n",
    "X_test2 = sequence.pad_sequences(X_test2, maxlen=max_slen2,padding='pre')\n",
    "X_test3 = tk3.texts_to_sequences(X_test3)\n",
    "X_test3 = sequence.pad_sequences(X_test3, maxlen=max_slen3,padding='pre')\n",
    "X_test4 = tk4.texts_to_sequences(X_test4)\n",
    "X_test4 = sequence.pad_sequences(X_test4, maxlen=max_slen4,padding='pre')\n",
    "\n",
    "test_result = model.predict([X_test1,X_test2,X_test3,X_test4])\n",
    "pos_prob = pd.DataFrame(test_result)[0]\n",
    "\n",
    "test['prob'] = pos_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>내용</th>\n",
       "      <th>키워드</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131292</td>\n",
       "      <td>\"아직도 금강산 개성공단으로 전력송출하는지 알고싶네요  억류자산도 우리자산인만큼  ...</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.851317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394932</td>\n",
       "      <td>그냥 컵짜파게티 사서 전자렌지에 이분 돌리고 쓰까주고 일분 돌리고 쓰까주고 일분 돌...</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.708149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146247</td>\n",
       "      <td>\"중부는 작년에 비하면 에어콘 하루도 안틀었는데 누군가 에어콘 회사랑 관련있나\"</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.015528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154049</td>\n",
       "      <td>아린 남동생 개잘생김 역시 유전자,,,</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.937515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>398373</td>\n",
       "      <td>섹스를 하는 이유는 쾌락을 만족하기 위함과 자신의 유전자를 확실히 남기기 위함 두가...</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.962102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439089</th>\n",
       "      <td>287055</td>\n",
       "      <td>\"G7이 돈모아서 보내주면 되겠네 지들은 산업개발 다해놓곤 남은 하지 말라고?? 내...</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.997893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439090</th>\n",
       "      <td>32285</td>\n",
       "      <td>오! 나 서면자주가는데ㅋㅋ잘하면 만나겠다 ㅋㅋㅋㅋ사하구는 태풍때문에 피해많았어???</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.141282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439091</th>\n",
       "      <td>261099</td>\n",
       "      <td>방법 찾으면 제게도 알려주시길 청합니당 ㅎㅎ(플라스틱 병 말고 보온병 같은 건 어떨...</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.149044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439092</th>\n",
       "      <td>333285</td>\n",
       "      <td>@auddms0106 헐우리도글케바뀜근데 노랭이가아니고핫핑크ㅋㅋㅋ존나커다란플라스틱달...</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.972412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439093</th>\n",
       "      <td>218691</td>\n",
       "      <td>\"쇼풍인듯 ㅋㅋㅋㅋ\"</td>\n",
       "      <td>기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...</td>\n",
       "      <td>0.959886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439094 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                                 내용  \\\n",
       "0           131292  \"아직도 금강산 개성공단으로 전력송출하는지 알고싶네요  억류자산도 우리자산인만큼  ...   \n",
       "1           394932  그냥 컵짜파게티 사서 전자렌지에 이분 돌리고 쓰까주고 일분 돌리고 쓰까주고 일분 돌...   \n",
       "2           146247       \"중부는 작년에 비하면 에어콘 하루도 안틀었는데 누군가 에어콘 회사랑 관련있나\"   \n",
       "3           154049                              아린 남동생 개잘생김 역시 유전자,,,   \n",
       "4           398373  섹스를 하는 이유는 쾌락을 만족하기 위함과 자신의 유전자를 확실히 남기기 위함 두가...   \n",
       "...            ...                                                ...   \n",
       "439089      287055  \"G7이 돈모아서 보내주면 되겠네 지들은 산업개발 다해놓곤 남은 하지 말라고?? 내...   \n",
       "439090       32285     오! 나 서면자주가는데ㅋㅋ잘하면 만나겠다 ㅋㅋㅋㅋ사하구는 태풍때문에 피해많았어???   \n",
       "439091      261099  방법 찾으면 제게도 알려주시길 청합니당 ㅎㅎ(플라스틱 병 말고 보온병 같은 건 어떨...   \n",
       "439092      333285  @auddms0106 헐우리도글케바뀜근데 노랭이가아니고핫핑크ㅋㅋㅋ존나커다란플라스틱달...   \n",
       "439093      218691                                        \"쇼풍인듯 ㅋㅋㅋㅋ\"   \n",
       "\n",
       "                                                      키워드      prob  \n",
       "0       기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.851317  \n",
       "1       기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.708149  \n",
       "2       기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.015528  \n",
       "3       기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.937515  \n",
       "4       기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.962102  \n",
       "...                                                   ...       ...  \n",
       "439089  기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.997893  \n",
       "439090  기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.141282  \n",
       "439091  기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.149044  \n",
       "439092  기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.972412  \n",
       "439093  기쁜|기쁨|기쁘다|이쁘다|예쁘다|어여쁘다|이쁨|어여쁨|예뿌다|이뿌다|신기|기대|존경...  0.959886  \n",
       "\n",
       "[439094 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/compat/_optional.py:106: UserWarning: Pandas requires version '0.9.8' or newer of 'xlsxwriter' (version '0.7.3' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "test = test.sort_values(['prob'],ascending=False)\n",
    "test = test.reset_index(drop=True)\n",
    "pos_test_result_df = test.loc[0:(n-1),['내용','prob']]\n",
    "pos_test_result_df.to_excel('./sub_pos_df.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
