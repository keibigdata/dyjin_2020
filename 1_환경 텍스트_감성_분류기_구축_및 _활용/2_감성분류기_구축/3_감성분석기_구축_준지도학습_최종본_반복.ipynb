{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113오류 제외\n",
      "1377오류 제외\n",
      "2623오류 제외\n",
      "2972오류 제외\n",
      "5965오류 제외\n",
      "10049오류 제외\n",
      "10249오류 제외\n",
      "12258오류 제외\n",
      "12409오류 제외\n",
      "14122오류 제외\n",
      "16189오류 제외\n",
      "17618오류 제외\n",
      "18487오류 제외\n",
      "19808오류 제외\n",
      "21931오류 제외\n",
      "23680오류 제외\n",
      "25685오류 제외\n",
      "27013오류 제외\n",
      "29914오류 제외\n",
      "32050오류 제외\n",
      "32144오류 제외\n",
      "32222오류 제외\n",
      "32652오류 제외\n",
      "32695오류 제외\n",
      "35033오류 제외\n",
      "44839오류 제외\n",
      "92958오류 제외\n",
      "137531오류 제외\n",
      "reading data complete\n",
      "reading data complete\n",
      "reading data complete\n",
      "reading data complete\n",
      "1344오류 제외\n",
      "5569오류 제외\n",
      "8770오류 제외\n",
      "Train on 181543 samples, validate on 9991 samples\n",
      "Epoch 1/20\n",
      "181543/181543 [==============================] - 2778s 15ms/step - loss: 0.3555 - accuracy: 0.8486 - val_loss: 0.4898 - val_accuracy: 0.7840\n",
      "Epoch 2/20\n",
      "181543/181543 [==============================] - 2768s 15ms/step - loss: 0.0981 - accuracy: 0.9595 - val_loss: 0.4646 - val_accuracy: 0.8025\n",
      "Epoch 3/20\n",
      "181543/181543 [==============================] - 2759s 15ms/step - loss: 0.0691 - accuracy: 0.9727 - val_loss: 0.5098 - val_accuracy: 0.7964\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18오류 제외\n",
      "4399오류 제외\n",
      "4569오류 제외\n",
      "6751오류 제외\n",
      "11454오류 제외\n",
      "12755오류 제외\n",
      "13887오류 제외\n",
      "14165오류 제외\n",
      "18358오류 제외\n",
      "19787오류 제외\n",
      "21704오류 제외\n",
      "22050오류 제외\n",
      "25146오류 제외\n",
      "25944오류 제외\n",
      "26535오류 제외\n",
      "26858오류 제외\n",
      "27097오류 제외\n",
      "27974오류 제외\n",
      "29138오류 제외\n",
      "29589오류 제외\n",
      "31712오류 제외\n",
      "37307오류 제외\n",
      "38060오류 제외\n",
      "38930오류 제외\n",
      "44839오류 제외\n",
      "92958오류 제외\n",
      "137531오류 제외\n",
      "reading data complete\n",
      "reading data complete\n",
      "reading data complete\n",
      "reading data complete\n",
      "1681오류 제외\n",
      "3204오류 제외\n",
      "6346오류 제외\n",
      "Train on 181544 samples, validate on 9991 samples\n",
      "Epoch 1/20\n",
      "126976/181544 [===================>..........] - ETA: 13:53 - loss: 0.4365 - accuracy: 0.7929"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.layers import Dense, Concatenate,Dropout, Input, Embedding, LSTM, Bidirectional,Flatten,SpatialDropout1D, CuDNNLSTM ,Conv1D,GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,SpatialDropout1D\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing import sequence\n",
    "from random import *\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,SpatialDropout1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import copy\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Random_state\n",
    "rs_list = [1]\n",
    "history_list = []\n",
    "for rs in rs_list:\n",
    "    print(rs)\n",
    "    # 트레이닝 테스트 데이터 분리\n",
    "\n",
    "    f1 = pd.read_excel(\"./data/emoji_final.xlsx\")\n",
    "    f1_x_list = f1['comments'].tolist()\n",
    "    f1_y_list = f1['label'].tolist()\n",
    "\n",
    "    f1_x_train, f1_x_test,f1_y_train,f1_y_test = train_test_split(f1_x_list,f1_y_list,test_size = 0.2,random_state=rs, stratify=f1_y_list)\n",
    "    f1 = pd.DataFrame(list(zip(f1_x_train,f1_y_train)),columns=['comments','label'])\n",
    "\n",
    "    # In[4]:\n",
    "\n",
    "    f2 = pd.read_excel(\"./data/sub_neg_neg_df.xlsx\")\n",
    "    f2['label'] = 1\n",
    "    f3 = pd.read_excel(\"./data/sub_pos_pos_df.xlsx\")\n",
    "    f3['label'] = 0\n",
    "\n",
    "\n",
    "    # In[5]:\n",
    "\n",
    "\n",
    "    f2 = f2[['comments','label']]\n",
    "    f3 = f3[['comments','label']]\n",
    "\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    f = pd.concat([f1,f2,f3],axis=0)\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    f = f.reset_index(drop=True)\n",
    "    f = f.drop_duplicates()\n",
    "\n",
    "    # # 전체 전처리 \n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    # 형태소 분석기\n",
    "\n",
    "    pos_tagger1 = Mecab()\n",
    "    pos_tagger2 = Twitter()\n",
    "\n",
    "    pos_tagger = pos_tagger1\n",
    "\n",
    "    fidx = []\n",
    "    t_result_list = []\n",
    "\n",
    "    t_list = f['comments']\n",
    "\n",
    "    # 형태소 분석이 어려운 샘플은 제외\n",
    "    for idx,t in enumerate(t_list):\n",
    "        try:\n",
    "\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "\n",
    "            pos = list(pd.DataFrame(pos_tagger1.pos(t))[1])\n",
    "            pos2 = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "\n",
    "            if len(pos) == 0 or len(pos2) == 0:\n",
    "                print(str(idx)+\"오류 제외\")\n",
    "            else :\n",
    "                fidx.append(idx)\n",
    "        except:\n",
    "            print(str(idx)+\"오류 제외\")\n",
    "\n",
    "    # 추가 전처리    \n",
    "    f = f.iloc[fidx,:]\n",
    "\n",
    "\n",
    "    # In[9]:\n",
    "\n",
    "\n",
    "    BASE_CODE, CHOSUNG, JUNGSUNG = 44032, 588, 28\n",
    "\n",
    "    # 초성 리스트. 00 ~ 18\n",
    "    CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "    # 중성 리스트. 00 ~ 20\n",
    "    JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "    # 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "    JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "    def convert(test_keyword):\n",
    "        split_keyword_list = list(test_keyword)\n",
    "        #print(split_keyword_list)\n",
    "\n",
    "        result = list()\n",
    "        for keyword in split_keyword_list:\n",
    "            # 한글 여부 check 후 분리\n",
    "           # print(keyword)\n",
    "            if re.match('.*[ㄱ-ㅎㅏ-ㅣ가-힣]+.*', keyword) is not None:\n",
    "                try:\n",
    "                    char_code = ord(keyword) - BASE_CODE\n",
    "                    char1 = int(char_code / CHOSUNG)\n",
    "                    if char1==0:  \n",
    "                        result.append('')\n",
    "                    result.append(CHOSUNG_LIST[char1])\n",
    "                   # print('초성 : {}'.format(CHOSUNG_LIST[char1]))\n",
    "                    char2 = int((char_code - (CHOSUNG * char1)) / JUNGSUNG)\n",
    "                    if char2==0:\n",
    "                        result.append('')\n",
    "                    result.append(JUNGSUNG_LIST[char2])\n",
    "                  #  print('중성 : {}'.format(JUNGSUNG_LIST[char2]))\n",
    "                    char3 = int((char_code - (CHOSUNG * char1) - (JUNGSUNG * char2)))\n",
    "                    if char3==0:\n",
    "                        result.append('')\n",
    "                    else:\n",
    "                        result.append(JONGSUNG_LIST[char3])\n",
    "                   # print('종성 : {}'.format(JONGSUNG_LIST[char3]))\n",
    "                except:\n",
    "                    result.append(keyword)\n",
    "            else:\n",
    "                result.append(keyword)\n",
    "        # result\n",
    "        return \"\".join(result)\n",
    "\n",
    "\n",
    "    # # 통합모델\n",
    "\n",
    "    # ## 형태소 분석 \n",
    "\n",
    "    # In[10]:\n",
    "\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    import os \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "    f1 = copy.deepcopy(f)\n",
    "\n",
    "    t_list = f1['comments'].tolist()\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    t_result_list = []\n",
    "\n",
    "    # 형태소 분석결과 아무것도 안나오는 경우\n",
    "    for idx,t in enumerate(t_list):\n",
    "        try:\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "\n",
    "            # 조사 제거\n",
    "            pos = list(pd.DataFrame(pos_tagger.pos(t))[1])\n",
    "            t = np.array(pos_tagger.morphs(t))\n",
    "            t = t.tolist()\n",
    "            t_result_list.append(\" \".join(t))\n",
    "\n",
    "        except:\n",
    "            print(t)\n",
    "            print(str(idx)+\" 제외\")\n",
    "\n",
    "    f1['comments'] = t_result_list\n",
    "\n",
    "    # label 교체\n",
    "    label_list1 = f1['label']\n",
    "\n",
    "    X_all1 = f1['comments'].tolist()\n",
    "    X_all_list1 = X_all1\n",
    "    Y_all_list1 = f1['label'].tolist()\n",
    "\n",
    "    X_train1, X_test1,Y_train1,Y_test1 = train_test_split(X_all_list1,Y_all_list1,test_size = 0.2,random_state=rs, stratify=Y_all_list1)\n",
    "\n",
    "    # 여기서 합쳐줘야댐 \n",
    "\n",
    "    # convert string Label into 0,1 columns (3개의 컬럼)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "\n",
    "    X_train1 = pd.DataFrame(X_train1)\n",
    "    Y_train1 = pd.DataFrame(Y_train1)\n",
    "    all_train1 = pd.concat([X_train1,Y_train1],axis=1)\n",
    "    all_train1.columns=['X','Y']\n",
    "\n",
    "\n",
    "    X_train1 = all_train1['X'].tolist()\n",
    "    Y_train1 = all_train1['Y'].tolist()\n",
    "    Y_train1 = pd.DataFrame(encoder.fit_transform(Y_train1))\n",
    "    Y_test1 = pd.DataFrame(encoder.fit_transform(Y_test1))\n",
    "\n",
    "    # Binary Label인 경우에만\n",
    "\n",
    "    Y_train1[1] = Y_train1[0]\n",
    "    Y_train1[0] = -1*(Y_train1[0]-1)\n",
    "    Y_test1[1] = Y_test1[0]\n",
    "    Y_test1[0] = -1*(Y_test1[0]-1)\n",
    "\n",
    "    max_slen1 = 200\n",
    "    embedding_vecor_length1 = 128\n",
    "\n",
    "    tk1 = keras.preprocessing.text.Tokenizer(lower=True, split=\" \")\n",
    "    tk1.fit_on_texts(list(X_train1))\n",
    "\n",
    "    X_all1 = X_train1 + X_test1\n",
    "    X_train1 = tk1.texts_to_sequences(X_train1)\n",
    "    X_test1 = tk1.texts_to_sequences(X_test1)\n",
    "    X_all1 = tk1.texts_to_sequences(X_all1)\n",
    "    #Y_all_arr = np.array(Y_all_df)\n",
    "\n",
    "\n",
    "    X_train1 = sequence.pad_sequences(X_train1, maxlen=max_slen1,padding='pre')\n",
    "    X_test1 = sequence.pad_sequences(X_test1, maxlen=max_slen1,padding='pre')\n",
    "    X_all1 = sequence.pad_sequences(X_all1, maxlen=max_slen1,padding='pre')\n",
    "\n",
    "    nwords1 = X_train1.max() + 1\n",
    "\n",
    "    print(\"reading data complete\")\n",
    "\n",
    "\n",
    "    # In[11]:\n",
    "\n",
    "\n",
    "    X_train1.shape\n",
    "\n",
    "\n",
    "    # ## 자모단위\n",
    "\n",
    "    # In[12]:\n",
    "\n",
    "\n",
    "    # 댓글 파일 로드\n",
    "\n",
    "    f2 = copy.deepcopy(f)\n",
    "\n",
    "    t_list = f2['comments'].tolist()\n",
    "    t_result_list = []\n",
    "\n",
    "\n",
    "    for t in t_list:\n",
    "        try:\n",
    "            t_result_list.append(t)\n",
    "        except:\n",
    "            print(str(idx)+\" 제외\")\n",
    "\n",
    "    # # 추가 전처리    \n",
    "\n",
    "    for i,s in enumerate(t_result_list):\n",
    "        s = str(s)\n",
    "        s = s.strip()\n",
    "        temp_list = s.split(\" \")\n",
    "\n",
    "       # 특수문자 제거  \n",
    "        s = re.sub('#', '', s)\n",
    "        s = re.sub('\\'', '', s)\n",
    "        s = re.sub('\\「', '', s)\n",
    "        s = re.sub('\\」', '', s)\n",
    "        s = re.sub('\\,', '', s)\n",
    "        s = re.sub('#', '', s)\n",
    "        s = re.sub('@', '', s)\n",
    "        s = re.sub('\\+', '', s)\n",
    "        s = re.sub('[A-z]', '', s)\n",
    "        s = re.sub('[0-9]', '', s)\n",
    "        s = s.strip()\n",
    "\n",
    "        t_result_list[i] = s\n",
    "\n",
    "    f2['comments'] = t_result_list\n",
    "\n",
    "\n",
    "    # label 2개로\n",
    "    label_list2 = f2['label']\n",
    "\n",
    "    max_slen2 = 500\n",
    "    embedding_vecor_length2 = 128\n",
    "\n",
    "    X_all2 = f2['comments']\n",
    "    X_all_list2 = X_all2.tolist()\n",
    "\n",
    "    for i, comment in enumerate(X_all_list2):\n",
    "        temp = convert(comment)\n",
    "\n",
    "        # 빈칸제거\n",
    "        temp = temp.replace(\" \",\"\")\n",
    "        temp2 = \"\"\n",
    "        for t in temp:\n",
    "            temp2 = temp2 + t +  \" \"\n",
    "        temp2 = temp2[0:-1]\n",
    "        X_all_list2[i] = temp2\n",
    "\n",
    "\n",
    "    f2['comments'] = X_all_list2\n",
    "    f2 = f2.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    X_all_list2 = f2['comments'].tolist()\n",
    "\n",
    "    # LABEL을 데이터 프레임 형태로\n",
    "    Y_all_list2 = f2['label'].tolist()\n",
    "    Y_all_df2 = pd.DataFrame(Y_all_list2)\n",
    "\n",
    "    # 숫자 매핑\n",
    "\n",
    "    nwords2 = 320\n",
    "\n",
    "    tk2 = keras.preprocessing.text.Tokenizer(num_words=nwords2,char_level=True,lower=False, split=\"\")\n",
    "\n",
    "    X_train2,X_test2,Y_train2,Y_test2 = train_test_split(X_all_list2,Y_all_list2,test_size = 0.2,random_state=rs,stratify=Y_all_list2)\n",
    "\n",
    "    tk2.fit_on_texts(X_train2)\n",
    "\n",
    "    X_train2 = tk2.texts_to_sequences(X_train2)\n",
    "    X_test2 = tk2.texts_to_sequences(X_test2)\n",
    "    X_all2 = tk2.texts_to_sequences(X_all2)\n",
    "\n",
    "\n",
    "    # convert string Label into 0,1 columns (3개의 컬럼)\n",
    "    encoder = LabelBinarizer()\n",
    "\n",
    "    Y_all_df2 = pd.DataFrame(encoder.fit_transform(Y_train2))\n",
    "    Y_all_df2[1] = Y_all_df2[0]\n",
    "    Y_all_df2[0] = -1*(Y_all_df2[0]-1)\n",
    "    Y_train2 = Y_all_df2\n",
    "\n",
    "    Y_all_df2 = pd.DataFrame(encoder.fit_transform(Y_test2))\n",
    "    Y_all_df2[1] = Y_all_df2[0]\n",
    "    Y_all_df2[0] = -1*(Y_all_df2[0]-1)\n",
    "    Y_test2 = Y_all_df2\n",
    "\n",
    "    X_train2 = sequence.pad_sequences(X_train2, maxlen=max_slen2,padding='pre')\n",
    "    X_test2 = sequence.pad_sequences(X_test2, maxlen=max_slen2,padding='pre')\n",
    "    X_all2 = sequence.pad_sequences(X_all2, maxlen=max_slen2,padding='pre')\n",
    "    nwords2 = X_train2.max() + 1\n",
    "    print(\"reading data complete\")\n",
    "\n",
    "\n",
    "    # # 한글자단위\n",
    "\n",
    "    # In[13]:\n",
    "\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "\n",
    "    f3 = copy.deepcopy(f)\n",
    "\n",
    "    t_list = f3['comments'].tolist()\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    t_result_list = []\n",
    "\n",
    "    # 형태소 분석결과 아무것도 안나오는 경우\n",
    "    for idx,t in enumerate(t_list):\n",
    "        try:\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "            t = t.strip()\n",
    "\n",
    "            # 조사 제거\n",
    "            t = t.replace(\" \", \"\") \n",
    "            t = list(t)\n",
    "            t_result_list.append(\" \".join(t))\n",
    "\n",
    "        except:\n",
    "            print(str(idx)+\" 제외\")\n",
    "\n",
    "\n",
    "    f3['comments'] = t_result_list\n",
    "\n",
    "    # label 교체\n",
    "    label_list3 = f3['label']\n",
    "\n",
    "    X_all3 = f3['comments'].tolist()\n",
    "    X_all_list3 = X_all3\n",
    "    Y_all_list3 = f3['label'].tolist()\n",
    "\n",
    "    X_train3, X_test3,Y_train3,Y_test3 = train_test_split(X_all_list3,Y_all_list3,test_size = 0.2,random_state=rs, stratify=Y_all_list3)\n",
    "\n",
    "\n",
    "    # convert string Label into 0,1 columns (3개의 컬럼)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "\n",
    "    X_train3 = pd.DataFrame(X_train3)\n",
    "    Y_train3 = pd.DataFrame(Y_train3)\n",
    "    all_train3 = pd.concat([X_train3,Y_train3],axis=1)\n",
    "    all_train3.columns=['X','Y']\n",
    "\n",
    "\n",
    "    X_train3 = all_train3['X'].tolist()\n",
    "    Y_train3 = all_train3['Y'].tolist()\n",
    "    Y_train3 = pd.DataFrame(encoder.fit_transform(Y_train3))\n",
    "    Y_test3 = pd.DataFrame(encoder.fit_transform(Y_test3))\n",
    "\n",
    "    # Binary Label인 경우에만\n",
    "\n",
    "    Y_train3[1] = Y_train3[0]\n",
    "    Y_train3[0] = -1*(Y_train3[0]-1)\n",
    "    Y_test3[1] = Y_test3[0]\n",
    "    Y_test3[0] = -1*(Y_test3[0]-1)\n",
    "\n",
    "    max_slen3 = 200\n",
    "    embedding_vecor_length3 = 128\n",
    "\n",
    "    tk3 = keras.preprocessing.text.Tokenizer(lower=True, split=\" \")\n",
    "    tk3.fit_on_texts(list(X_train3))\n",
    "\n",
    "    X_all3 = X_train3 + X_test3\n",
    "    X_train3 = tk3.texts_to_sequences(X_train3)\n",
    "    X_test3 = tk3.texts_to_sequences(X_test3)\n",
    "    X_all3 = tk3.texts_to_sequences(X_all3)\n",
    "    #Y_all_arr = np.array(Y_all_df)\n",
    "\n",
    "\n",
    "    X_train3 = sequence.pad_sequences(X_train3, maxlen=max_slen3,padding='pre')\n",
    "    X_test3 = sequence.pad_sequences(X_test3, maxlen=max_slen3,padding='pre')\n",
    "    X_all3 = sequence.pad_sequences(X_all3, maxlen=max_slen3,padding='pre')\n",
    "\n",
    "    nwords3 = X_train3.max() + 1\n",
    "\n",
    "    print(\"reading data complete\")\n",
    "\n",
    "\n",
    "    # # 형태소 분석 (Twitter)\n",
    "\n",
    "    # In[14]:\n",
    "\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    import os \n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "    # Random_state\n",
    "\n",
    "    f4 = copy.deepcopy(f)\n",
    "\n",
    "    t_list = f4['comments'].tolist()\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    t_result_list = []\n",
    "\n",
    "    # 형태소 분석결과 아무것도 안나오는 경우\n",
    "    for idx,t in enumerate(t_list):\n",
    "        try:\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "\n",
    "\n",
    "            # 조사 제거\n",
    "            pos = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "            t = np.array(pos_tagger2.morphs(t))\n",
    "            t = t.tolist()\n",
    "            t_result_list.append(\" \".join(t))\n",
    "\n",
    "        except:\n",
    "            print(t)\n",
    "            print(str(idx)+\" 제외\")\n",
    "\n",
    "    f4['comments'] = t_result_list\n",
    "\n",
    "    # label 교체\n",
    "    label_list4 = f4['label']\n",
    "\n",
    "    X_all4 = f4['comments'].tolist()\n",
    "    X_all_list4 = X_all4\n",
    "    Y_all_list4 = f4['label'].tolist()\n",
    "\n",
    "    X_train4, X_test4,Y_train4,Y_test4 = train_test_split(X_all_list4,Y_all_list4,test_size = 0.2,random_state=rs, stratify=Y_all_list4)\n",
    "\n",
    "\n",
    "    # convert string Label into 0,1 columns (3개의 컬럼)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "\n",
    "    X_train4 = pd.DataFrame(X_train4)\n",
    "    Y_train4 = pd.DataFrame(Y_train4)\n",
    "    all_train4 = pd.concat([X_train4,Y_train4],axis=1)\n",
    "    all_train4.columns=['X','Y']\n",
    "\n",
    "\n",
    "    X_train4 = all_train4['X'].tolist()\n",
    "    Y_train4 = all_train4['Y'].tolist()\n",
    "    Y_train4 = pd.DataFrame(encoder.fit_transform(Y_train4))\n",
    "    Y_test4 = pd.DataFrame(encoder.fit_transform(Y_test4))\n",
    "\n",
    "    # Binary Label인 경우에만\n",
    "\n",
    "    Y_train4[1] = Y_train4[0]\n",
    "    Y_train4[0] = -1*(Y_train4[0]-1)\n",
    "    Y_test4[1] = Y_test4[0]\n",
    "    Y_test4[0] = -1*(Y_test4[0]-1)\n",
    "\n",
    "    max_slen4 = 200\n",
    "    embedding_vecor_length4 = 128\n",
    "\n",
    "    tk4 = keras.preprocessing.text.Tokenizer(lower=True, split=\" \")\n",
    "    tk4.fit_on_texts(list(X_train4))\n",
    "\n",
    "    X_all4 = X_train4 + X_test4\n",
    "    X_train4 = tk4.texts_to_sequences(X_train4)\n",
    "    X_test4 = tk4.texts_to_sequences(X_test4)\n",
    "    X_all4 = tk4.texts_to_sequences(X_all4)\n",
    "    #Y_all_arr = np.array(Y_all_df)\n",
    "\n",
    "\n",
    "    X_train4 = sequence.pad_sequences(X_train4, maxlen=max_slen4,padding='pre')\n",
    "    X_test4 = sequence.pad_sequences(X_test4, maxlen=max_slen4,padding='pre')\n",
    "    X_all4 = sequence.pad_sequences(X_all4, maxlen=max_slen4,padding='pre')\n",
    "\n",
    "    nwords4 = X_train4.max() + 1\n",
    "\n",
    "    print(\"reading data complete\")\n",
    "\n",
    "\n",
    "    # In[15]:\n",
    "\n",
    "\n",
    "    f_X_test = pd.Series(f1_x_test)\n",
    "    f_Y_test = pd.Series(f1_y_test)\n",
    "\n",
    "\n",
    "    # In[16]:\n",
    "\n",
    "\n",
    "    idx_list = []\n",
    "\n",
    "    # 형태소 분석이 어려운 샘플은 제외\n",
    "    for idx,t in enumerate(f_X_test):\n",
    "        try:\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "\n",
    "            pos = list(pd.DataFrame(pos_tagger1.pos(t))[1])\n",
    "            pos2 = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "\n",
    "            if len(pos) == 0 or len(pos2) == 0:\n",
    "                print(str(idx)+\"오류 제외\")\n",
    "            else :\n",
    "                idx_list.append(idx)\n",
    "        except:\n",
    "            print(str(idx)+\"오류 제외\")\n",
    "\n",
    "    f_X_test =  f_X_test[idx_list].tolist()\n",
    "    f_Y_test =  f_Y_test[idx_list].tolist()\n",
    "\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    t_result_list = []\n",
    "\n",
    "    # 형태소 분석결과 아무것도 안나오는 경우\n",
    "    for idx,t in enumerate(f_X_test):\n",
    "        try:\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "\n",
    "            # 조사 제거\n",
    "            pos = list(pd.DataFrame(pos_tagger.pos(t))[1])\n",
    "            t = np.array(pos_tagger.morphs(t))\n",
    "            t = t.tolist()\n",
    "            t_result_list.append(\" \".join(t))\n",
    "\n",
    "        except:\n",
    "            print(t)\n",
    "            print(str(idx)+\" 제외\")\n",
    "\n",
    "    f_X_test1 = t_result_list\n",
    "\n",
    "\n",
    "    # In[18]:\n",
    "\n",
    "\n",
    "    t_result_list = []\n",
    "    for i,s in enumerate(f_X_test):\n",
    "        s = str(s)\n",
    "        s = s.strip()\n",
    "        temp_list = s.split(\" \")\n",
    "\n",
    "       # 특수문자 제거  \n",
    "        s = re.sub('#', '', s)\n",
    "        s = re.sub('\\'', '', s)\n",
    "        s = re.sub('\\「', '', s)\n",
    "        s = re.sub('\\」', '', s)\n",
    "        s = re.sub('\\,', '', s)\n",
    "        s = re.sub('#', '', s)\n",
    "        s = re.sub('@', '', s)\n",
    "        s = re.sub('\\+', '', s)\n",
    "        s = re.sub('[A-z]', '', s)\n",
    "        s = re.sub('[0-9]', '', s)\n",
    "        s = s.strip()\n",
    "\n",
    "        t_result_list.append(s)\n",
    "\n",
    "    X_all_list2 = t_result_list\n",
    "\n",
    "    for i, comment in enumerate(X_all_list2):\n",
    "        temp = convert(comment)\n",
    "\n",
    "        # 빈칸제거\n",
    "        temp = temp.replace(\" \",\"\")\n",
    "        temp2 = \"\"\n",
    "        for t in temp:\n",
    "            temp2 = temp2 + t +  \" \"\n",
    "        temp2 = temp2[0:-1]\n",
    "        X_all_list2[i] = temp2\n",
    "\n",
    "    f_X_test2 = X_all_list2\n",
    "\n",
    "\n",
    "    # In[19]:\n",
    "\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    t_result_list = []\n",
    "\n",
    "    # 형태소 분석결과 아무것도 안나오는 경우\n",
    "    for idx,t in enumerate(f_X_test):\n",
    "        try:\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "            t = t.strip()\n",
    "\n",
    "            # 조사 제거\n",
    "            t = t.replace(\" \", \"\") \n",
    "            t = list(t)\n",
    "            t_result_list.append(\" \".join(t))\n",
    "\n",
    "        except:\n",
    "            print(str(idx)+\" 제외\")\n",
    "\n",
    "    f_X_test3 = t_result_list\n",
    "\n",
    "\n",
    "    # In[20]:\n",
    "\n",
    "\n",
    "    # Mecab 형태소 분석기 사용\n",
    "    t_result_list = []\n",
    "\n",
    "    # 형태소 분석결과 아무것도 안나오는 경우\n",
    "    for idx,t in enumerate(f_X_test):\n",
    "        try:\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('\\'', '', t)\n",
    "            t = re.sub('\\「', '', t)\n",
    "            t = re.sub('\\」', '', t)\n",
    "            t = re.sub('\\,', '', t)\n",
    "            t = re.sub('#', '', t)\n",
    "            t = re.sub('@', '', t)\n",
    "            t = re.sub('\\+', '', t)\n",
    "            t = re.sub('[A-z]', '', t)\n",
    "            t = re.sub('[0-9]', '', t)\n",
    "\n",
    "            # 조사 제거\n",
    "            pos = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "            t = np.array(pos_tagger2.morphs(t))\n",
    "            t = t.tolist()\n",
    "            t_result_list.append(\" \".join(t))        \n",
    "        except:\n",
    "            print(t)\n",
    "            #print(str(idx)+\" 제외\")\n",
    "\n",
    "    f_X_test4 = t_result_list\n",
    "\n",
    "\n",
    "    # In[21]:\n",
    "\n",
    "\n",
    "    f_X_test1 = tk1.texts_to_sequences(f_X_test1)\n",
    "    f_X_test1 = sequence.pad_sequences(f_X_test1, maxlen=max_slen1,padding='pre')\n",
    "    f_X_test2 = tk2.texts_to_sequences(f_X_test2)\n",
    "    f_X_test2 = sequence.pad_sequences(f_X_test2, maxlen=max_slen2,padding='pre')\n",
    "    f_X_test3 = tk3.texts_to_sequences(f_X_test3)\n",
    "    f_X_test3 = sequence.pad_sequences(f_X_test3, maxlen=max_slen3,padding='pre')\n",
    "    f_X_test4 = tk4.texts_to_sequences(f_X_test4)\n",
    "    f_X_test4 = sequence.pad_sequences(f_X_test4, maxlen=max_slen4,padding='pre')\n",
    "\n",
    "\n",
    "    # In[22]:\n",
    "\n",
    "\n",
    "    nwords1 = X_train1.max() + 2\n",
    "    nwords2 = X_train2.max() + 2\n",
    "    nwords3 = X_train3.max() + 2\n",
    "    nwords4 = X_train4.max() + 2\n",
    "\n",
    "\n",
    "    # In[23]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In[24]:\n",
    "\n",
    "\n",
    "    class TestCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, test_data):\n",
    "            self.test_data = test_data\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            x, y = self.test_data\n",
    "            loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "            print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
    "\n",
    "\n",
    "    # In[25]:\n",
    "\n",
    "\n",
    "    # Binary Label인 경우에만\n",
    "    f_Y_test = pd.DataFrame(f_Y_test)\n",
    "    f_Y_test[1] = f_Y_test[0]\n",
    "    f_Y_test[0] = -1*(f_Y_test[0]-1)\n",
    "\n",
    "\n",
    "    # In[26]:\n",
    "\n",
    "\n",
    "    Y_all = pd.concat([Y_train1,Y_test1],axis=0)\n",
    "\n",
    "\n",
    "    # In[27]:\n",
    "\n",
    "\n",
    "    # Bi-LSTM (형태소, 문자단위)\n",
    "\n",
    "    \n",
    "    NUM_FILTERS = 64\n",
    "    NUM_WORDS = 3 \n",
    "    early_stopping = EarlyStopping(monitor='val_loss',patience=1,restore_best_weights=True)\n",
    "\n",
    "    with K.tf.device('/gpu:0'):\n",
    "        model1 = Sequential()\n",
    "        model1.add(Embedding(nwords1, embedding_vecor_length1, input_length=max_slen1))\n",
    "        model1.add(SpatialDropout1D(0.2))\n",
    "        model1.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\")) \n",
    "        model1.add(Dropout(0.2)) \n",
    "        model1.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))    \n",
    "        model1.add(Dropout(0.2))\n",
    "        model1.add(Dense(64,activation='relu'))\n",
    "\n",
    "        model2 = Sequential()\n",
    "        model2.add(Embedding(nwords2, embedding_vecor_length2, input_length=max_slen2))\n",
    "        model2.add(SpatialDropout1D(0.2)) \n",
    "        model2.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\")) \n",
    "        model2.add(Dropout(0.2)) \n",
    "        model2.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        model2.add(Dropout(0.2))\n",
    "        model2.add(Dense(64, activation='relu'))\n",
    "\n",
    "        model3 = Sequential()\n",
    "        model3.add(Embedding(nwords3, embedding_vecor_length3, input_length=max_slen3))\n",
    "        model3.add(SpatialDropout1D(0.2)) \n",
    "        model3.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\")) \n",
    "        model3.add(Dropout(0.2)) \n",
    "        model3.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        model3.add(Dropout(0.2))\n",
    "        model3.add(Dense(64, activation='relu'))\n",
    "\n",
    "        model4 = Sequential()\n",
    "        model4.add(Embedding(nwords4, embedding_vecor_length4, input_length=max_slen4))\n",
    "        model4.add(SpatialDropout1D(0.2)) \n",
    "        model4.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\")) \n",
    "        model4.add(Dropout(0.2)) \n",
    "        model4.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "        model4.add(Dropout(0.2))\n",
    "        model4.add(Dense(64, activation='relu'))\n",
    "\n",
    "        model_input1 = Input((max_slen1,))\n",
    "        model_input2 = Input((max_slen2,))\n",
    "        model_input3 = Input((max_slen3,))\n",
    "        model_input4 = Input((max_slen4,))\n",
    "\n",
    "        out1 = model1(model_input1)   \n",
    "        out2 = model2(model_input2)\n",
    "        out3 = model3(model_input3)\n",
    "        out4 = model4(model_input4)\n",
    "\n",
    "        out = Concatenate()([out1,out2,out3,out4])\n",
    "        out = Dense(2, activation='softmax')(out)\n",
    "\n",
    "        model = Model([model_input1,model_input2,model_input3,model_input4],out)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #history= model.fit([X_train1,X_train2,X_train3,X_train4],Y_train1, epochs=20, batch_size=4096,validation_data=([X_test1,X_test2,X_test3,X_test4],Y_test1),shuffle=False,callbacks=[early_stopping,TestCallback(([f_X_test1,f_X_test2,f_X_test3,f_X_test4], f_Y_test))])\n",
    "        #history= model.fit([X_train1,X_train2,X_train3,X_train4],Y_train1, epochs=20, batch_size=4096,validation_data=([f_X_test1,f_X_test2,f_X_test3,f_X_test4], f_Y_test),shuffle=False,callbacks=[early_stopping])\n",
    "        history= model.fit([X_all1,X_all2,X_all3,X_all4],Y_all, epochs=20, batch_size=4096,validation_data=([f_X_test1,f_X_test2,f_X_test3,f_X_test4], f_Y_test),shuffle=False,callbacks=[early_stopping])\n",
    "    \n",
    "    history_list.append(history.history)\n",
    "    model.save('senti_model_'+str(rs) +'.h5')\n",
    "    # saving\n",
    "    with open('tk1_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(tk1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('tk2_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(tk2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('tk3_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(tk3, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('tk4_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(tk4, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tk1_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tk1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tk2_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tk2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tk3_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tk3, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('tk4_' + str(rs) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(tk4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
