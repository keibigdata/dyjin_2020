{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.layers import Dense, Concatenate,Dropout, Input, Embedding, LSTM, Bidirectional,Flatten,SpatialDropout1D, CuDNNLSTM ,Conv1D,GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,SpatialDropout1D\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing import sequence\n",
    "from random import *\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,SpatialDropout1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import copy\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger1 = Mecab()\n",
    "pos_tagger2 = Twitter()\n",
    "\n",
    "pos_tagger = pos_tagger1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = '/home/dyjin/2019_환경_텍스트_수집_소스코드/data/'\n",
    "file_list = os.listdir(file_dir)\n",
    "file_list = [file for file in file_list if file.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "for f in file_list:\n",
    "    fpath = file_dir + f\n",
    "    print(fpath)\n",
    "    temp = pd.read_csv(fpath)\n",
    "    try:\n",
    "        result_df = pd.concat([result_df,temp])\n",
    "    except:\n",
    "        print(\"a\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_result_df = result_df[['날짜','내용','종류','키워드']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_result_df = sub_result_df.drop_duplicates(['내용'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_result_df = sub_result_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = sub_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df['내용'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CODE, CHOSUNG, JUNGSUNG = 44032, 588, 28\n",
    "\n",
    "# 초성 리스트. 00 ~ 18\n",
    "CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "# 중성 리스트. 00 ~ 20\n",
    "JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "# 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "def convert(test_keyword):\n",
    "    split_keyword_list = list(test_keyword)\n",
    "    #print(split_keyword_list)\n",
    "\n",
    "    result = list()\n",
    "    for keyword in split_keyword_list:\n",
    "        # 한글 여부 check 후 분리\n",
    "       # print(keyword)\n",
    "        if re.match('.*[ㄱ-ㅎㅏ-ㅣ가-힣]+.*', keyword) is not None:\n",
    "            try:\n",
    "                char_code = ord(keyword) - BASE_CODE\n",
    "                char1 = int(char_code / CHOSUNG)\n",
    "                if char1==0:  \n",
    "                    result.append('')\n",
    "                result.append(CHOSUNG_LIST[char1])\n",
    "               # print('초성 : {}'.format(CHOSUNG_LIST[char1]))\n",
    "                char2 = int((char_code - (CHOSUNG * char1)) / JUNGSUNG)\n",
    "                if char2==0:\n",
    "                    result.append('')\n",
    "                result.append(JUNGSUNG_LIST[char2])\n",
    "              #  print('중성 : {}'.format(JUNGSUNG_LIST[char2]))\n",
    "                char3 = int((char_code - (CHOSUNG * char1) - (JUNGSUNG * char2)))\n",
    "                if char3==0:\n",
    "                    result.append('')\n",
    "                else:\n",
    "                    result.append(JONGSUNG_LIST[char3])\n",
    "               # print('종성 : {}'.format(JONGSUNG_LIST[char3]))\n",
    "            except:\n",
    "                result.append(keyword)\n",
    "        else:\n",
    "            result.append(keyword)\n",
    "    # result\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    if idx % 10000 == 0 :\n",
    "        print(\"굿\")\n",
    "        \n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        pos = list(pd.DataFrame(pos_tagger.pos(t))[1])\n",
    "        t = np.array(pos_tagger.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))\n",
    "    except:\n",
    "        t_result_list.append(\"중립\")\n",
    "        \n",
    "X_test1 = t_result_list\n",
    "\n",
    "t_result_list = []\n",
    "for i,s in enumerate(X_test):\n",
    "    s = str(s)\n",
    "    s = s.strip()\n",
    "    temp_list = s.split(\" \")\n",
    "    \n",
    "   # 특수문자 제거  \n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('\\'', '', s)\n",
    "    s = re.sub('\\「', '', s)\n",
    "    s = re.sub('\\」', '', s)\n",
    "    s = re.sub('\\,', '', s)\n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('@', '', s)\n",
    "    s = re.sub('\\+', '', s)\n",
    "    s = re.sub('[A-z]', '', s)\n",
    "    s = re.sub('[0-9]', '', s)\n",
    "    s = s.strip()\n",
    "    \n",
    "    t_result_list.append(s)\n",
    "\n",
    "X_all_list2 = t_result_list\n",
    "\n",
    "for i, comment in enumerate(X_all_list2):\n",
    "    temp = convert(comment)\n",
    "    \n",
    "    # 빈칸제거\n",
    "    temp = temp.replace(\" \",\"\")\n",
    "    temp2 = \"\"\n",
    "    for t in temp:\n",
    "        temp2 = temp2 + t +  \" \"\n",
    "    temp2 = temp2[0:-1]\n",
    "    X_all_list2[i] = temp2\n",
    "\n",
    "X_test2 = X_all_list2\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        t = t.strip()\n",
    "        \n",
    "        # 조사 제거\n",
    "        t = t.replace(\" \", \"\") \n",
    "        t = list(t)\n",
    "        t_result_list.append(\" \".join(t))\n",
    "        \n",
    "    except:\n",
    "        t_result_list.append(\"중립\")\n",
    "\n",
    "X_test3 = t_result_list\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        # 조사 제거\n",
    "        pos = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "        t = np.array(pos_tagger2.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))        \n",
    "    except:\n",
    "        print(t)\n",
    "        t_result_list.append(\"중립\")\n",
    "\n",
    "X_test4 = t_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_slen1 = 200\n",
    "max_slen2 = 500\n",
    "max_slen3 = 200\n",
    "max_slen4 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('./model/tk1_1.pickle', 'rb') as handle:\n",
    "    tk1 = pickle.load(handle)\n",
    "with open('./model/tk2_1.pickle', 'rb') as handle:\n",
    "    tk2 = pickle.load(handle)\n",
    "with open('./model/tk3_1.pickle', 'rb') as handle:\n",
    "    tk3 = pickle.load(handle)\n",
    "with open('./model/tk4_1.pickle', 'rb') as handle:\n",
    "    tk4 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = tk1.texts_to_sequences(X_test1)\n",
    "X_test1 = sequence.pad_sequences(X_test1, maxlen=max_slen1,padding='pre')\n",
    "X_test2 = tk2.texts_to_sequences(X_test2)\n",
    "X_test2 = sequence.pad_sequences(X_test2, maxlen=max_slen2,padding='pre')\n",
    "X_test3 = tk3.texts_to_sequences(X_test3)\n",
    "X_test3 = sequence.pad_sequences(X_test3, maxlen=max_slen3,padding='pre')\n",
    "X_test4 = tk4.texts_to_sequences(X_test4)\n",
    "X_test4 = sequence.pad_sequences(X_test4, maxlen=max_slen4,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "with tf.device('/cpu:0'):\n",
    "    model = load_model('./model/senti_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    test_result = model.predict([X_test1,X_test2,X_test3,X_test4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result.to_csv(\"twitter_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
