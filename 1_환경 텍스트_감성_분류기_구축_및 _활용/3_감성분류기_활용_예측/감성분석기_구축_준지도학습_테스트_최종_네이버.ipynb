{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.layers import Dense, Concatenate,Dropout, Input, Embedding, LSTM, Bidirectional,Flatten,SpatialDropout1D, CuDNNLSTM ,Conv1D,GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from ckonlpy.tag import Twitter\n",
    "from konlpy.tag import Mecab\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,SpatialDropout1D\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import keras\n",
    "import keras.preprocessing.text\n",
    "from keras.preprocessing import sequence\n",
    "from random import *\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,SpatialDropout1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import copy\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "pos_tagger1 = Mecab()\n",
    "pos_tagger2 = Twitter()\n",
    "\n",
    "pos_tagger = pos_tagger1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = pd.read_excel(\"./data/testset.xlsx\",header=None,sheet_name=\"쓰레기폐기물\")\n",
    "#thres = 0.6\n",
    "test_df = pd.read_csv(\"/media/Data/Naver_Comments/result/naver_comments_long.csv\")\n",
    "fname = \"네이버_결과.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns=['no','data','title','comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df['comments'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CODE, CHOSUNG, JUNGSUNG = 44032, 588, 28\n",
    "\n",
    "# 초성 리스트. 00 ~ 18\n",
    "CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "# 중성 리스트. 00 ~ 20\n",
    "JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "# 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "def convert(test_keyword):\n",
    "    split_keyword_list = list(test_keyword)\n",
    "    #print(split_keyword_list)\n",
    "\n",
    "    result = list()\n",
    "    for keyword in split_keyword_list:\n",
    "        # 한글 여부 check 후 분리\n",
    "       # print(keyword)\n",
    "        if re.match('.*[ㄱ-ㅎㅏ-ㅣ가-힣]+.*', keyword) is not None:\n",
    "            try:\n",
    "                char_code = ord(keyword) - BASE_CODE\n",
    "                char1 = int(char_code / CHOSUNG)\n",
    "                if char1==0:  \n",
    "                    result.append('')\n",
    "                result.append(CHOSUNG_LIST[char1])\n",
    "               # print('초성 : {}'.format(CHOSUNG_LIST[char1]))\n",
    "                char2 = int((char_code - (CHOSUNG * char1)) / JUNGSUNG)\n",
    "                if char2==0:\n",
    "                    result.append('')\n",
    "                result.append(JUNGSUNG_LIST[char2])\n",
    "              #  print('중성 : {}'.format(JUNGSUNG_LIST[char2]))\n",
    "                char3 = int((char_code - (CHOSUNG * char1) - (JUNGSUNG * char2)))\n",
    "                if char3==0:\n",
    "                    result.append('')\n",
    "                else:\n",
    "                    result.append(JONGSUNG_LIST[char3])\n",
    "               # print('종성 : {}'.format(JONGSUNG_LIST[char3]))\n",
    "            except:\n",
    "                result.append(keyword)\n",
    "        else:\n",
    "            result.append(keyword)\n",
    "    # result\n",
    "    return \"\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료1\n",
      "완료2\n"
     ]
    }
   ],
   "source": [
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "        \n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        pos = list(pd.DataFrame(pos_tagger.pos(t))[1])\n",
    "        t = np.array(pos_tagger.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))\n",
    "    except:\n",
    "        t_result_list.append(\"중립\")\n",
    "        \n",
    "X_test1 = t_result_list\n",
    "print(\"완료1\")\n",
    "t_result_list = []\n",
    "for i,s in enumerate(X_test):\n",
    "    s = str(s)\n",
    "    s = s.strip()\n",
    "    temp_list = s.split(\" \")\n",
    "    \n",
    "   # 특수문자 제거  \n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('\\'', '', s)\n",
    "    s = re.sub('\\「', '', s)\n",
    "    s = re.sub('\\」', '', s)\n",
    "    s = re.sub('\\,', '', s)\n",
    "    s = re.sub('#', '', s)\n",
    "    s = re.sub('@', '', s)\n",
    "    s = re.sub('\\+', '', s)\n",
    "    s = re.sub('[A-z]', '', s)\n",
    "    s = re.sub('[0-9]', '', s)\n",
    "    s = s.strip()\n",
    "    \n",
    "    t_result_list.append(s)\n",
    "\n",
    "X_all_list2 = t_result_list\n",
    "\n",
    "for i, comment in enumerate(X_all_list2):\n",
    "    temp = convert(comment)\n",
    "    \n",
    "    # 빈칸제거\n",
    "    temp = temp.replace(\" \",\"\")\n",
    "    temp2 = \"\"\n",
    "    for t in temp:\n",
    "        temp2 = temp2 + t +  \" \"\n",
    "    temp2 = temp2[0:-1]\n",
    "    X_all_list2[i] = temp2\n",
    "\n",
    "X_test2 = X_all_list2\n",
    "print(\"완료2\")\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        t = t.strip()\n",
    "        \n",
    "        # 조사 제거\n",
    "        t = t.replace(\" \", \"\") \n",
    "        t = list(t)\n",
    "        t_result_list.append(\" \".join(t))\n",
    "        \n",
    "    except:\n",
    "        t_result_list.append(\"중립\")\n",
    "\n",
    "X_test3 = t_result_list\n",
    "\n",
    "# Mecab 형태소 분석기 사용\n",
    "t_result_list = []\n",
    "\n",
    "# 형태소 분석결과 아무것도 안나오는 경우\n",
    "for idx,t in enumerate(X_test):\n",
    "    try:\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('\\'', '', t)\n",
    "        t = re.sub('\\「', '', t)\n",
    "        t = re.sub('\\」', '', t)\n",
    "        t = re.sub('\\,', '', t)\n",
    "        t = re.sub('#', '', t)\n",
    "        t = re.sub('@', '', t)\n",
    "        t = re.sub('\\+', '', t)\n",
    "        t = re.sub('[A-z]', '', t)\n",
    "        t = re.sub('[0-9]', '', t)\n",
    "        \n",
    "        # 조사 제거\n",
    "        pos = list(pd.DataFrame(pos_tagger2.pos(t))[1])\n",
    "        t = np.array(pos_tagger2.morphs(t))\n",
    "        t = t.tolist()\n",
    "        t_result_list.append(\" \".join(t))        \n",
    "    except:\n",
    "        print(t)\n",
    "        t_result_list.append(\"중립\")\n",
    "\n",
    "X_test4 = t_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_slen1 = 200\n",
    "max_slen2 = 500\n",
    "max_slen3 = 200\n",
    "max_slen4 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('./model/tk1_1.pickle', 'rb') as handle:\n",
    "    tk1 = pickle.load(handle)\n",
    "with open('./model/tk2_1.pickle', 'rb') as handle:\n",
    "    tk2 = pickle.load(handle)\n",
    "with open('./model/tk3_1.pickle', 'rb') as handle:\n",
    "    tk3 = pickle.load(handle)\n",
    "with open('./model/tk4_1.pickle', 'rb') as handle:\n",
    "    tk4 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cd445655970c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_test1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_slen1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_slen2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtexts_to_sequences_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    310\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0mvect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "X_test1 = tk1.texts_to_sequences(X_test1)\n",
    "X_test1 = sequence.pad_sequences(X_test1, maxlen=max_slen1,padding='pre')\n",
    "X_test2 = tk2.texts_to_sequences(X_test2)\n",
    "X_test2 = sequence.pad_sequences(X_test2, maxlen=max_slen2,padding='pre')\n",
    "X_test3 = tk3.texts_to_sequences(X_test3)\n",
    "X_test3 = sequence.pad_sequences(X_test3, maxlen=max_slen3,padding='pre')\n",
    "X_test4 = tk4.texts_to_sequences(X_test4)\n",
    "X_test4 = sequence.pad_sequences(X_test4, maxlen=max_slen4,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "with tf.device('/cpu:0'):\n",
    "    model = load_model('./model/senti_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_X_test1 = X_test1[1:1000000]\n",
    "sub_X_test2 = X_test2[1:1000000]\n",
    "sub_X_test3 = X_test3[1:1000000]\n",
    "sub_X_test4 = X_test4[1:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    test_result = model.predict([sub_X_test1,sub_X_test2,sub_X_test3,sub_X_test4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = pd.DataFrame(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999999, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv(\"naver_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
